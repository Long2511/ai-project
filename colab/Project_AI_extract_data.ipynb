{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Long2511/ai-project/blob/main/colab/Project_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) Installs dependencies"
      ],
      "metadata": {
        "id": "QuzgSx61-4sY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0337f7"
      },
      "source": [
        "import sys, subprocess, os\n",
        "def sh(cmd): print(cmd); subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "# CUDA 12.1-compatible torch for Colab\n",
        "!python3 -m pip -q install --index-url https://download.pytorch.org/whl/cu121 \"torch==2.5.1\" \"torchaudio==2.5.1\" \"torchvision==0.20.1\"\n",
        "# Core libs: ColPali, transformers, qdrant-client (multi-vector), OCR, PDF\n",
        "!python3 -m pip -q install \"transformers>=4.53.1,<4.54.0\" colpali-engine==0.3.12 \"qdrant-client>=1.7.3,<2\" accelerate sentencepiece pdf2image pytesseract\n",
        "# System deps for OCR/PDF\n",
        "sh('apt-get -y update && apt-get -y install tesseract-ocr poppler-utils')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Mount Google Drive"
      ],
      "metadata": {
        "id": "uheW89mL-xTS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ba36d4e"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"[colab] Drive mounted at /content/drive\")\n",
        "except Exception as e:\n",
        "    print(\"[note] Not in Colab or Drive mount failed:\", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Config"
      ],
      "metadata": {
        "id": "285bRs0U-u2a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd29b356"
      },
      "source": [
        "SOURCE_DIR         = \"/content/drive/MyDrive/Project-AI/PDF-Data\"\n",
        "MODEL_NAME         = \"vidore/colpali-v1.2-hf\"\n",
        "\n",
        "# Outputs\n",
        "PAGES_JSONL        = \"/content/clinical_cases_index.jsonl\"\n",
        "CASES_JSONL        = \"/content/clinical_cases_cases.jsonl\"\n",
        "STRUCT_JSONL       = \"/content/clinical_cases_cases_structured.jsonl\"\n",
        "SFT_EXTRACT_JSONL  = \"/content/clinical_cases_extract_sft.jsonl\"\n",
        "SFT_DX_JSONL       = \"/content/clinical_cases_dx_sft.jsonl\"\n",
        "\n",
        "# Qdrant collections\n",
        "COLLECTION_PAGES   = \"tropical_cases_colpali_pages\"\n",
        "COLLECTION_CASES   = \"tropical_cases_colpali_cases\"\n",
        "VECTOR_SIZE        = 128  # ColPali subvector dim\n",
        "\n",
        "# Toggles\n",
        "INDEX_PAGES        = True\n",
        "INDEX_CASES        = True\n",
        "ENABLE_OCR         = True\n",
        "ENABLE_PDF         = True\n",
        "BATCH              = 2      # embedding batch size\n",
        "MAX_FILES          = None   # set small int to smoke-test\n",
        "\n",
        "# Qdrant remote (REST-only)\n",
        "QDRANT_HOST        = \"165.22.56.15\"\n",
        "QDRANT_PORT        = 6334    # QRPc",
        "QDRANT_API_KEY     = os.getenv(\"QDRANT_API_KEY\") or None\n",
        "QDRANT_TIMEOUT     = 1200.0  # large to be safe\n",
        "UPSERT_BATCH       = 12      # points per upsert() call; keep small for reliability\n",
        "UPSERT_MAX_RETRIES = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Imports"
      ],
      "metadata": {
        "id": "PHFyi_SP-slh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cb0faf4"
      },
      "source": [
        "import re, json, glob, hashlib, io, time\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from transformers import ColPaliForRetrieval, ColPaliProcessor\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "try:\n",
        "    import pytesseract\n",
        "except Exception as e:\n",
        "    print(\"[warn] OCR disabled:\", e); ENABLE_OCR=False\n",
        "\n",
        "try:\n",
        "    from pdf2image import convert_from_path\n",
        "except Exception as e:\n",
        "    print(\"[warn] PDF→image disabled:\", e); ENABLE_PDF=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Scan / OCR / sectionize"
      ],
      "metadata": {
        "id": "2XT01Nyk-qSs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9504e4e8"
      },
      "source": [
        "def list_media_recursive(root: str) -> List[str]:\n",
        "    pats = [\"**/*.png\",\"**/*.jpg\",\"**/*.jpeg\",\"**/*.pdf\"]\n",
        "    out=[]; [out.extend(glob.glob(os.path.join(root, p), recursive=True)) for p in pats]\n",
        "    return sorted(out)\n",
        "\n",
        "def load_image(path: str) -> Image.Image:\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "def parse_page_number_from_path(path: str, default: int = 1) -> int:\n",
        "    if \"#page=\" in path:\n",
        "        try: return int(path.split(\"#page=\")[1])\n",
        "        except: return default\n",
        "    m = re.search(r\"_page_(\\d+)\", path, re.I)\n",
        "    return int(m.group(1)) if m else default\n",
        "\n",
        "def derive_case_id_from_path(path: str) -> int:\n",
        "    base = path.split(\"#page=\")[0]\n",
        "    stem = os.path.splitext(os.path.basename(base))[0]\n",
        "    stem = re.sub(r\"_page_\\d+$\", \"\", stem, flags=re.I)\n",
        "    m = re.match(r\"^\\s*(\\d+)\\b\", stem)\n",
        "    if m: return int(m.group(1))\n",
        "    return int(hashlib.sha1(stem.encode()).hexdigest()[:6], 16)\n",
        "\n",
        "def guess_title_from_path(p: str) -> str:\n",
        "    base = os.path.basename(p.split(\"#page=\")[0])\n",
        "    base = re.sub(r\"_page_\\d+$\", \"\", os.path.splitext(base)[0], flags=re.I)\n",
        "    return re.sub(r\"[-_]+\", \" \", base).strip()\n",
        "\n",
        "SECTION_HEADERS = [\n",
        "    \"Clinical Presentation\",\"History\",\"Clinical Findings\",\"Laboratory Findings\",\n",
        "    \"Laboratory Results\",\"Laboratory Investigations\",\"Additional Investigations\",\n",
        "    \"Investigations\",\"Questions\",\"Discussion\",\"Answer to Question\",\n",
        "    \"Summary Box\",\"Further Reading\",\"The Case Continued\",\"The Case Continued…\"\n",
        "]\n",
        "SEC_RX = re.compile(rf\"(^|\\n)\\s*({'|'.join([re.escape(h) for h in SECTION_HEADERS])})\\s*\\n\", re.I)\n",
        "\n",
        "def sectionize(text: str) -> Dict[str,str]:\n",
        "    parts = SEC_RX.split(text or \"\")\n",
        "    out, cur, buf = {}, \"body\", []\n",
        "    for chunk in parts:\n",
        "        if chunk and chunk.strip() in SECTION_HEADERS:\n",
        "            if buf: out[cur] = \"\\n\".join(buf).strip(); buf=[]\n",
        "            cur = chunk.strip()\n",
        "        else:\n",
        "            if chunk: buf.append(chunk)\n",
        "    if buf: out[cur] = \"\\n\".join(buf).strip()\n",
        "    return out\n",
        "\n",
        "def ocr_text_from_image(img: Image.Image) -> str:\n",
        "    if not ENABLE_OCR: return \"\"\n",
        "    try: return pytesseract.image_to_string(img)\n",
        "    except Exception: return \"\"\n",
        "\n",
        "def load_all_pages(root: str) -> Tuple[List[Image.Image], List[Dict[str, Any]]]:\n",
        "    files = list_media_recursive(root)\n",
        "    print(f\"[scan] total candidates under {root} -> {len(files)}\")\n",
        "    out_imgs, out_meta = [], []\n",
        "    for p in files:\n",
        "        if MAX_FILES and len(out_imgs) >= MAX_FILES: break\n",
        "        ext = os.path.splitext(p)[1].lower()\n",
        "\n",
        "        if ext == \".pdf\" and ENABLE_PDF:\n",
        "            try:\n",
        "                pages = convert_from_path(p, dpi=220)\n",
        "                for i, im in enumerate(pages, start=1):\n",
        "                    if MAX_FILES and len(out_imgs) >= MAX_FILES: break\n",
        "                    out_imgs.append(im)\n",
        "                    out_meta.append({\n",
        "                        \"case_title\": guess_title_from_path(p),\n",
        "                        \"path\": f\"{p}#page={i}\",\n",
        "                        \"page_number\": i,\n",
        "                        \"case_id\": derive_case_id_from_path(p),\n",
        "                        \"source\": \"PDF page\",\n",
        "                        \"modality\": \"page_image\"\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(\"[pdf] failed ->\", p, \"|\", e)\n",
        "\n",
        "        elif ext in (\".png\",\".jpg\",\".jpeg\"):\n",
        "            try:\n",
        "                im = load_image(p)\n",
        "                out_imgs.append(im)\n",
        "                out_meta.append({\n",
        "                    \"case_title\": guess_title_from_path(p),\n",
        "                    \"path\": p,\n",
        "                    \"page_number\": parse_page_number_from_path(p, 1),\n",
        "                    \"case_id\": derive_case_id_from_path(p),\n",
        "                    \"source\": \"image\",\n",
        "                    \"modality\": \"page_image\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(\"[img] failed ->\", p, \"|\", e)\n",
        "\n",
        "    print(f\"[scan] pages prepared: {len(out_imgs)}\")\n",
        "    return out_imgs, out_meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) ColPali embed"
      ],
      "metadata": {
        "id": "4NWj4dqF-kt3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50696f01"
      },
      "source": [
        "def load_colpali(device=None):\n",
        "    device = device or (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[env] device={device}\\n[env] loading ColPali…\")\n",
        "\n",
        "    model = ColPaliForRetrieval.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
        "    ).eval()\n",
        "    model.to(device)\n",
        "\n",
        "    processor = ColPaliProcessor.from_pretrained(MODEL_NAME)\n",
        "    print(\"[env] ColPali loaded.\")\n",
        "    return model, processor, device\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_images(model, processor, device, pil_images: List[Image.Image]):\n",
        "    batch = processor(images=pil_images).to(device)\n",
        "    emb = model(**batch).embeddings  # [B, N, 128] multivectors\n",
        "    return [e.to(\"cpu\").float().tolist() for e in emb]  # List[List[List[float]]]\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_queries(model, processor, device, queries: List[str]):\n",
        "    batch = processor(text=queries).to(device)\n",
        "    emb = model(**batch).embeddings\n",
        "    return [e.to(\"cpu\").float().tolist() for e in emb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Qdrant connect (REST-only)"
      ],
      "metadata": {
        "id": "lwvDcwTP-hl0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0511e37b"
      },
      "source": [
        "def connect_qdrant_rest():\n",
        "    url = f\"http://{QDRANT_HOST}:{QDRANT_PORT}\"\n",
        "    client = QdrantClient(\n",
        "        url=url,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "        timeout=QDRANT_TIMEOUT,\n",
        "        prefer_grpc=False,  # <- HARD disable gRPC\n",
        "    )\n",
        "    # sanity call\n",
        "    client.get_collections()\n",
        "    print(f\"[qdrant] connected (REST-only): {url}\")\n",
        "    return client\n",
        "\n",
        "def ensure_collection(client: QdrantClient, name: str):\n",
        "    try:\n",
        "        exists = client.collection_exists(name)\n",
        "    except Exception:\n",
        "        # REST fallback path if needed\n",
        "        try:\n",
        "            client.http.collections_api.get_collection(name)\n",
        "            exists = True\n",
        "        except Exception:\n",
        "            exists = False\n",
        "\n",
        "    if not exists:\n",
        "        print(f\"[qdrant] creating collection: {name}\")\n",
        "        client.create_collection(\n",
        "            collection_name=name,\n",
        "            vectors_config=models.VectorParams(\n",
        "                size=VECTOR_SIZE,\n",
        "                distance=models.Distance.COSINE,\n",
        "                multivector_config=models.MultiVectorConfig(\n",
        "                    comparator=models.MultiVectorComparator.MAX_SIM\n",
        "                ),\n",
        "            ),\n",
        "            on_disk_payload=True,\n",
        "            hnsw_config=models.HnswConfigDiff(m=32, ef_construct=128),\n",
        "            optimizers_config=models.OptimizersConfigDiff(default_segment_number=2),\n",
        "        )\n",
        "    else:\n",
        "        print(f\"[qdrant] collection exists: {name}\")\n",
        "\n",
        "# Deterministic 63-bit IDs (so re-runs are true updates)\n",
        "def stable_point_id(key: str) -> int:\n",
        "    h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
        "    return int(h[:15], 16) & ((1<<63)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Upsert helper (uses client.upsert in batches)"
      ],
      "metadata": {
        "id": "625Pcw6k-eo7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1f71ba4"
      },
      "source": [
        "def upsert_points(client: QdrantClient, name: str, vectors, payloads, id_keys):\n",
        "    assert len(vectors) == len(payloads) == len(id_keys)\n",
        "    total = len(id_keys)\n",
        "    print(f\"[qdrant] upserting {total} points → {name} with client.upsert, batch={UPSERT_BATCH}\")\n",
        "    for i in range(0, total, UPSERT_BATCH):\n",
        "        chunk_vecs  = vectors[i:i+UPSERT_BATCH]\n",
        "        chunk_pl    = payloads[i:i+UPSERT_BATCH]\n",
        "        chunk_ids   = id_keys[i:i+UPSERT_BATCH]\n",
        "\n",
        "        points = [\n",
        "            models.PointStruct(\n",
        "                id=stable_point_id(k),\n",
        "                vector=v,           # multivector: List[List[float]] (subvector size=128)\n",
        "                payload=p\n",
        "            ) for v, p, k in zip(chunk_vecs, chunk_pl, chunk_ids)\n",
        "        ]\n",
        "\n",
        "        # retry with exponential backoff\n",
        "        for attempt in range(UPSERT_MAX_RETRIES):\n",
        "            try:\n",
        "                client.upsert(collection_name=name, points=points, wait=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == UPSERT_MAX_RETRIES - 1:\n",
        "                    print(f\"[qdrant] upsert batch {i//UPSERT_BATCH+1} FAILED permanently:\", repr(e))\n",
        "                    raise\n",
        "                sleep_s = 1.5 * (2 ** attempt)\n",
        "                print(f\"[qdrant] upsert batch {i//UPSERT_BATCH+1} retry {attempt+1} in {sleep_s:.1f}s →\", repr(e))\n",
        "                time.sleep(sleep_s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Build PAGE-level index + JSONL"
      ],
      "metadata": {
        "id": "rDY7nFt2-cQ-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f27284c4"
      },
      "source": [
        "def build_pages_index(root: str):\n",
        "    model, processor, device = load_colpali()\n",
        "    client = connect_qdrant_rest()\n",
        "    imgs, meta = load_all_pages(root)\n",
        "    if not imgs:\n",
        "        raise RuntimeError(\"No pages prepared. Check SOURCE_DIR and file types (png/jpg/pdf).\")\n",
        "\n",
        "    if INDEX_PAGES:\n",
        "        ensure_collection(client, COLLECTION_PAGES)\n",
        "        print(\"[embed] pages…\")\n",
        "        all_vecs = []\n",
        "        for i in tqdm(range(0, len(imgs), BATCH)):\n",
        "            vecs = embed_images(model, processor, device, imgs[i:i+BATCH])  # list of multivectors\n",
        "            all_vecs.extend(vecs)\n",
        "\n",
        "        # Deterministic IDs from the page path\n",
        "        ids = [f\"page::{m['path']}\" for m in meta]\n",
        "        upsert_points(client, COLLECTION_PAGES, all_vecs, meta, ids)\n",
        "\n",
        "    print(\"[jsonl] writing per-page:\", PAGES_JSONL)\n",
        "    with open(PAGES_JSONL, \"w\", encoding=\"utf-8\") as fh:\n",
        "        for im, m in zip(imgs, meta):\n",
        "            rec = dict(m)\n",
        "            txt = ocr_text_from_image(im)\n",
        "            if txt:\n",
        "                rec[\"ocr_text\"] = txt\n",
        "                rec[\"sections\"] = sectionize(txt)\n",
        "            fh.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"[done] pages → {PAGES_JSONL}\")\n",
        "    return client, (model, processor, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Merge PAGES → unique CASES"
      ],
      "metadata": {
        "id": "bHnchrii-UkP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09277536"
      },
      "source": [
        "def merge_pages_to_cases(pages_jsonl: str, out_cases_jsonl: str) -> str:\n",
        "    print(\"[merge] reading:\", pages_jsonl)\n",
        "    groups: Dict[int, List[Dict[str, Any]]] = {}\n",
        "    with open(pages_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            rec = json.loads(line)\n",
        "            cid = rec.get(\"case_id\")\n",
        "            try: cid = int(cid)\n",
        "            except: cid = derive_case_id_from_path(rec[\"path\"])\n",
        "            rec[\"case_id\"] = cid\n",
        "            groups.setdefault(cid, []).append(rec)\n",
        "\n",
        "    n_cases = 0\n",
        "    with open(out_cases_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
        "        for cid, records in groups.items():\n",
        "            pages = sorted(records, key=lambda r: int(r.get(\"page_number\", parse_page_number_from_path(r[\"path\"],1))))\n",
        "            title = pages[0].get(\"case_title\") or guess_title_from_path(pages[0][\"path\"])\n",
        "            merged_texts = [p.get(\"ocr_text\") for p in pages if p.get(\"ocr_text\")]\n",
        "            merged_text = \"\\n\\n\".join(\n",
        "                (f\"--- PAGE {p.get('page_number', parse_page_number_from_path(p['path'],1))} ---\\n{p.get('ocr_text','')}\".strip())\n",
        "                for p in pages if p.get(\"ocr_text\")\n",
        "            ).strip() if merged_texts else None\n",
        "\n",
        "            merged_sections: Dict[str,str] = {}\n",
        "            for p in pages:\n",
        "                secs = p.get(\"sections\") or {}\n",
        "                for k, v in secs.items():\n",
        "                    if not v: continue\n",
        "                    merged_sections[k] = (merged_sections.get(k, \"\") + (\"\\n\\n\" if merged_sections.get(k) else \"\") + v).strip()\n",
        "\n",
        "            case_obj = {\n",
        "                \"case_id\": cid,\n",
        "                \"case_title\": title,\n",
        "                \"n_pages\": len(pages),\n",
        "                \"page_paths\": [p[\"path\"] for p in pages],\n",
        "                \"merged\": {\"ocr_text\": merged_text, \"sections\": merged_sections or None}\n",
        "            }\n",
        "            out.write(json.dumps(case_obj, ensure_ascii=False) + \"\\n\")\n",
        "            n_cases += 1\n",
        "\n",
        "    print(f\"[done] cases → {out_cases_jsonl} | total unique cases: {n_cases}\")\n",
        "    return out_cases_jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Structured extractor"
      ],
      "metadata": {
        "id": "XtcAELNI-Q5h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e835ebc4"
      },
      "source": [
        "SYMPTOM_LEXICON = {\n",
        "    \"fever\":[\"fever\",\"pyrexia\",\"febrile\"], \"cough\":[\"cough\"], \"headache\":[\"headache\",\"cephalgia\"],\n",
        "    \"rash\":[\"rash\",\"maculopapular\",\"petechiae\",\"urticaria\",\"vesicular\"], \"diarrhea\":[\"diarrhea\",\"diarrhoea\",\"loose stools\"],\n",
        "    \"vomiting\":[\"vomit\",\"emesis\"], \"abdominal pain\":[\"abdominal pain\",\"abd pain\",\"stomach pain\"],\n",
        "    \"jaundice\":[\"jaundice\",\"icterus\"], \"weight loss\":[\"weight loss\"], \"night sweats\":[\"night sweats\"],\n",
        "    \"myalgia\":[\"myalgia\",\"muscle pain\"], \"arthralgia\":[\"arthralgia\",\"joint pain\"], \"dyspnea\":[\"dyspnea\",\"shortness of breath\"],\n",
        "    \"chest pain\":[\"chest pain\"], \"pruritus\":[\"pruritus\",\"itch\"], \"hematuria\":[\"hematuria\",\"blood in urine\"],\n",
        "    \"hematochezia\":[\"hematochezia\"], \"melena\":[\"melena\",\"melaena\"], \"conjunctivitis\":[\"conjunctivitis\",\"red eyes\"],\n",
        "    \"proptosis\":[\"proptosis\",\"eye bulging\"], \"bleeding\":[\"bleeding\",\"hemorrhage\",\"haemorrhage\",\"gum bleeding\"],\n",
        "    \"confusion\":[\"confusion\",\"altered mental state\"], \"lymphadenopathy\":[\"lymphadenopathy\",\"swollen nodes\"],\n",
        "    \"hepatosplenomegaly\":[\"hepatosplenomegaly\",\"hepatomegaly\",\"splenomegaly\"], \"ulcer\":[\"ulcer\",\"eschar\",\"chancre\"],\n",
        "    \"lesion\":[\"lesion\",\"plaque\",\"nodule\",\"papule\",\"pustule\"], \"itchy boil\":[\"boil\",\"furuncle\",\"myiasis\"],\n",
        "}\n",
        "NEGATION_RX = re.compile(r\"\\b(no|not|denies?|without|absence of)\\b\", re.I)\n",
        "VITALS_PATTERNS = {\n",
        "    \"temperature_c\": r\"(?:temp(?:erature)?|t)\\s*[:=]?\\s*(\\d{1,2}(?:\\.\\d)?)\\s*°?\\s*c\",\n",
        "    \"temperature_f\": r\"(?:temp(?:erature)?|t)\\s*[:=]?\\s*(\\d{2,3}(?:\\.\\d)?)\\s*°?\\s*f\",\n",
        "    \"hr_bpm\":        r\"(?:hr|heart\\s*rate|pulse)\\s*[:=]?\\s*(\\d{2,3})\\s*bpm?\",\n",
        "    \"bp\":            r\"(?:bp|blood\\s*pressure)\\s*[:=]?\\s*(\\d{2,3})\\s*/\\s*(\\d{2,3})\",\n",
        "    \"rr\":            r\"(?:rr|respiratory\\s*rate)\\s*[:=]?\\s*(\\d{1,2})\\s*/?min|\\brr\\s*(\\d{1,2})\\b\",\n",
        "    \"spo2\":          r\"(?:spo2|sat(?:uration)?|oxygen\\s*saturation)\\s*[:=]?\\s*(\\d{2,3})\\s*%\",\n",
        "    \"height_cm\":     r\"(?:height|ht)\\s*[:=]?\\s*(\\d{2,3})\\s*cm\",\n",
        "    \"weight_kg\":     r\"(?:weight|wt)\\s*[:=]?\\s*(\\d{1,3}(?:\\.\\d)?)\\s*kg\",\n",
        "}\n",
        "LAB_PATTERNS = {\n",
        "    \"hb_g_dl\":          (r\"\\b(?:hb|hemoglobin|haemoglobin)\\s*[:=]?\\s*(\\d{1,2}(?:\\.\\d)?)\\s*g/?dl\", \"g/dL\"),\n",
        "    \"wbc_10e9_l\":       (r\"\\b(?:wbc|white\\s*blood\\s*cell[s]?)\\s*[:=]?\\s*(\\d{1,2}(?:\\.\\d)?)\\s*x?\\s*10\\^?9\\s*/?\\s*l\", \"10^9/L\"),\n",
        "    \"plt_10e9_l\":       (r\"\\b(?:plt|platelet[s]?)\\s*[:=]?\\s*(\\d{2,3}(?:\\.\\d+)?)\\s*x?\\s*10\\^?9\\s*/?\\s*l\", \"10^9/L\"),\n",
        "    \"crp_mg_l\":         (r\"\\bcrp\\s*[:=]?\\s*(\\d{1,3}(?:\\.\\d)?)\\s*mg/?l\", \"mg/L\"),\n",
        "    \"esr_mm_h\":         (r\"\\besr\\s*[:=]?\\s*(\\d{1,3})\\s*mm/?h\", \"mm/h\"),\n",
        "    \"alt_u_l\":          (r\"\\balt\\s*[:=]?\\s*(\\d{1,4})\\s*u/?l\", \"U/L\"),\n",
        "    \"ast_u_l\":          (r\"\\bast\\s*[:=]?\\s*(\\d{1,4})\\s*u/?l\", \"U/L\"),\n",
        "    \"bilirubin_umol_l\": (r\"\\bbilirubin\\s*[:=]?\\s*(\\d{1,4})\\s*(?:µ?mol/?l|umol/?l)\", \"µmol/L\"),\n",
        "    \"creatinine_umol_l\":(r\"\\bcreatinine\\s*[:=]?\\s*(\\d{1,4})\\s*(?:µ?mol/?l|umol/?l)\", \"µmol/L\"),\n",
        "    \"sodium_mmol_l\":    (r\"\\b(?:na|sodium)\\s*[:=]?\\s*(\\d{2,3})\\s*mmol/?l\", \"mmol/L\"),\n",
        "    \"potassium_mmol_l\": (r\"\\b(?:k|potassium)\\s*[:=]?\\s*(\\d\\.\\d|\\d{1,2})\\s*mmol/?l\", \"mmol/L\"),\n",
        "    \"glucose_mmol_l\":   (r\"\\bglucose\\s*[:=]?\\s*(\\d{1,2}(?:\\.\\d)?)\\s*mmol/?l\", \"mmol/L\"),\n",
        "}\n",
        "MICRO_PATTERNS = [\n",
        "    (r\"\\bthick\\s*smear\\b.*\\b(positive|negative)\\b\", \"malaria_thick_smear\"),\n",
        "    (r\"\\bthin\\s*smear\\b.*\\b(positive|negative)\\b\",  \"malaria_thin_smear\"),\n",
        "    (r\"\\brdt\\b.*\\b(positive|negative)\\b\",            \"malaria_RDT\"),\n",
        "    (r\"\\bhiv\\b.*\\b(positive|negative)\\b\",            \"HIV_test\"),\n",
        "    (r\"\\bbrucella\\b.*\\b(agglutination|serology|pcr|culture)\\b.*\\b(positive|negative)\\b\", \"Brucella_test\"),\n",
        "    (r\"\\bdengue\\b.*\\b(ns1|igg|igm|pcr)\\b.*\\b(positive|negative)\\b\", \"Dengue_test\"),\n",
        "    (r\"\\bblood\\s*culture[s]?\\b.*\\b(?:for\\s+)?([A-Z][a-zA-Z]+)\\b.*\\b(positive|negative)\\b\",\"blood_culture\"),\n",
        "]\n",
        "IMAGING_KEYS = [\"x-ray\",\"cxr\",\"ultrasound\",\"u/s\",\"ct\",\"mri\"]\n",
        "\n",
        "def find_symptoms(text: str) -> List[str]:\n",
        "    out=set(); t=\" \"+(text or \"\").lower()+\" \"\n",
        "    for canon, syns in SYMPTOM_LEXICON.items():\n",
        "        for s in syns:\n",
        "            for m in re.finditer(rf\"\\b{s}\\b\", t):\n",
        "                window=t[max(0,m.start()-25):m.start()]\n",
        "                if NEGATION_RX.search(window):\n",
        "                    continue\n",
        "                out.add(canon); break\n",
        "    return sorted(out)\n",
        "\n",
        "def parse_vitals(text: str) -> Dict[str, Any]:\n",
        "    t=(text or \"\").lower(); out={}\n",
        "    m=re.search(VITALS_PATTERNS[\"temperature_c\"],t)\n",
        "    if m: out[\"temperature_c\"]=float(m.group(1))\n",
        "    m=re.search(VITALS_PATTERNS[\"temperature_f\"],t)\n",
        "    if m and \"temperature_c\" not in out: out[\"temperature_c\"]=round((float(m.group(1))-32)*5/9,1)\n",
        "    m=re.search(VITALS_PATTERNS[\"hr_bpm\"],t);  out[\"hr_bpm\"]=int(m.group(1)) if m else None\n",
        "    m=re.search(VITALS_PATTERNS[\"bp\"],t);      out[\"bp_mmHg\"]=f\"{m.group(1)}/{m.group(2)}\" if m else None\n",
        "    m=re.search(VITALS_PATTERNS[\"rr\"],t);      out[\"rr_min\"]=int(m.group(1) or m.group(2)) if m else None\n",
        "    m=re.search(VITALS_PATTERNS[\"spo2\"],t);    out[\"spo2_pct\"]=int(m.group(1)) if m else None\n",
        "    m=re.search(VITALS_PATTERNS[\"height_cm\"],t); out[\"height_cm\"]=int(m.group(1)) if m else None\n",
        "    m=re.search(VITALS_PATTERNS[\"weight_kg\"],t); out[\"weight_kg\"]=float(m.group(1)) if m else None\n",
        "    if out.get(\"height_cm\") and out.get(\"weight_kg\"):\n",
        "        h=out[\"height_cm\"]/100.0; out[\"bmi\"]=round(out[\"weight_kg\"]/ (h*h),1)\n",
        "    return {k:v for k,v in out.items() if v is not None}\n",
        "\n",
        "def parse_labs(text: str) -> Dict[str, Dict[str, Any]]:\n",
        "    t=(text or \"\").lower(); labs={}\n",
        "    for key,(rx,unit) in LAB_PATTERNS.items():\n",
        "        m=re.search(rx,t)\n",
        "        if m:\n",
        "            try: val=float(m.group(1).replace(\",\",\"\"))\n",
        "            except: continue\n",
        "            labs[key]={\"value\":val,\"unit\":unit}\n",
        "    return labs\n",
        "\n",
        "def parse_micro(text: str) -> List[Dict[str, Any]]:\n",
        "    t=(text or \"\").lower(); out=[]\n",
        "    for rx,name in MICRO_PATTERNS:\n",
        "        for m in re.finditer(rx,t,re.I):\n",
        "            g=m.groups()\n",
        "            if name==\"blood_culture\":\n",
        "                out.append({\"test\":name,\"organism\":g[0],\"result\":g[1].lower()})\n",
        "            else:\n",
        "                out.append({\"test\":name,\"result\":g[-1].lower()})\n",
        "    return out\n",
        "\n",
        "def extract_imaging(text: str) -> List[str]:\n",
        "    lines=(text or \"\").splitlines(); hits=[]\n",
        "    for i,l in enumerate(lines):\n",
        "        if any(k in l.lower() for k in IMAGING_KEYS):\n",
        "            hits.append(\" \".join(lines[i:i+3]).strip())\n",
        "    return hits[:10]\n",
        "\n",
        "def extract_demographics(text: str) -> Dict[str, Any]:\n",
        "    out={}\n",
        "    m=re.search(r\"(\\d{1,3})\\s*[-–]?\\s*year[- ]old\", text or \"\", re.I)\n",
        "    if m: out[\"age\"]=int(m.group(1))\n",
        "    tl=(text or \"\").lower()\n",
        "    if any(w in tl for w in [\"female\",\"woman\",\"girl\"]): out[\"sex\"]=\"female\"\n",
        "    if any(w in tl for w in [\"male\",\"man\",\"boy\"]): out.setdefault(\"sex\",\"male\")\n",
        "    if re.search(r\"\\b(pregnan(t|cy))\\b\", tl): out[\"pregnant\"]=True\n",
        "    m=re.search(r\"\\bfrom\\s+([A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z]+)*)\", text or \"\")\n",
        "    if m: out[\"from_location\"]=m.group(1)\n",
        "    travels=re.findall(r\"\\b(?:returned|travel(?:ed|led)?|migrant|expatriate)\\s+(?:from|to)\\s+([A-Z][A-Za-z]+(?:\\s+[A-Za-z]+)*)\", text or \"\")\n",
        "    if travels: out[\"travel\"]=list(dict.fromkeys(travels))\n",
        "    if \"hiv\" in tl:\n",
        "        stat=\"unknown\"\n",
        "        if re.search(r\"hiv.*positive\", tl): stat=\"positive\"\n",
        "        if re.search(r\"hiv.*negative\", tl): stat=\"negative\"\n",
        "        out[\"hiv_status\"]=stat\n",
        "    return out\n",
        "\n",
        "def extract_diagnoses(full_text: str, sections: Dict[str,str]) -> Dict[str, Any]:\n",
        "    DIAG_KEYS = {\n",
        "        \"final\":[r\"\\bfinal\\s*diagnosis\\b\", r\"\\bdefinitive\\s*diagnosis\\b\", r\"\\bdiagnosis:\\b\"],\n",
        "    \"provisional\":[r\"\\bprovisional\\s*diagnosis\\b\", r\"\\bimpression\\b\"],\n",
        "        \"differential\":[r\"\\bdifferential[s]?\\b\", r\"\\bdifferential\\s*diagnoses?\\b\"],\n",
        "    }\n",
        "    out={\"provisional\":None,\"differential\":None,\"final\":None}\n",
        "    t=full_text or \"\"\n",
        "    for k,patterns in DIAG_KEYS.items():\n",
        "        for p in patterns:\n",
        "            m=re.search(p+r\".{0,30}[:]?\\s*(.+)\", t, re.I)\n",
        "            if m:\n",
        "                val=re.split(r\"\\n|\\.  \", m.group(1).strip())[0]\n",
        "                out[k]=val; break\n",
        "    if not out[\"final\"]:\n",
        "        disc=(sections or {}).get(\"Discussion\",\"\") or \"\"\n",
        "        m=re.search(r\"\\bdiagnos(e|is|ed)\\b.*?:?\\s*(.+)\", disc, re.I)\n",
        "        if m: out[\"final\"]=m.group(2).split(\"\\n\")[0].strip()\n",
        "    return out\n",
        "\n",
        "def extract_management(text: str) -> Dict[str, Any]:\n",
        "    tl=(text or \"\").lower()\n",
        "    rx=r\"(?:treated|given|started|therapy|administered)\\s+(?:with\\s+)?([A-Za-z][A-Za-z0-9\\- ]+)\"\n",
        "    meds=[m.group(1).strip() for m in re.finditer(rx, text or \"\", re.I)]\n",
        "    meds=list(dict.fromkeys(meds)) or None\n",
        "    outcome=None\n",
        "    if re.search(r\"\\bimproved|recovered|resolved|discharged\\b\", tl): outcome=\"improved\"\n",
        "    if re.search(r\"\\bdied|death|fatal\\b\", tl): outcome=\"died\"\n",
        "    return {\"medications\":meds,\"outcome\":outcome}\n",
        "\n",
        "def build_structured_cases(cases_jsonl: str, out_jsonl: str) -> str:\n",
        "    n=0\n",
        "    with open(cases_jsonl,\"r\",encoding=\"utf-8\") as fin, \\\n",
        "         open(out_jsonl,\"w\",encoding=\"utf-8\") as fout:\n",
        "        for line in fin:\n",
        "            case=json.loads(line)\n",
        "            merged=case.get(\"merged\") or {}\n",
        "            text=merged.get(\"ocr_text\") or \"\"\n",
        "            secs=merged.get(\"sections\") or sectionize(text)\n",
        "\n",
        "            hx = secs.get(\"History\",\"\") or \"\"\n",
        "            cf = secs.get(\"Clinical Findings\",\"\") or \"\"\n",
        "            labs_text = secs.get(\"Laboratory Findings\",\"\") or secs.get(\"Laboratory Results\",\"\") or secs.get(\"Laboratory Investigations\",\"\") or \"\"\n",
        "            inv_text  = secs.get(\"Additional Investigations\",\"\") or secs.get(\"Investigations\",\"\") or \"\"\n",
        "            disc = secs.get(\"Discussion\",\"\") or \"\"\n",
        "            summary = secs.get(\"Summary Box\",\"\") or \"\"\n",
        "\n",
        "            demographics = extract_demographics(text + \"\\n\" + hx)\n",
        "            vitals       = parse_vitals(hx + \"\\n\" + cf)\n",
        "            symptoms     = find_symptoms(hx + \"\\n\" + cf)\n",
        "            labs         = parse_labs(labs_text + \"\\n\" + inv_text)\n",
        "            microbiology = parse_micro(labs_text + \"\\n\" + inv_text)\n",
        "            imaging      = extract_imaging(inv_text + \"\\n\" + disc)\n",
        "            diagnoses    = extract_diagnoses(text + \"\\n\" + disc + \"\\n\" + summary, secs)\n",
        "            management   = extract_management(disc + \"\\n\" + summary)\n",
        "\n",
        "            signs=[]\n",
        "            for key in [\"rash\",\"lymphadenopathy\",\"hepatosplenomegaly\",\"ulcer\",\"lesion\",\"proptosis\",\"jaundice\"]:\n",
        "                if key in symptoms and re.search(rf\"\\b{key}\\b\", (cf or \"\").lower()):\n",
        "                    signs.append(key)\n",
        "            signs=sorted(set(signs))\n",
        "\n",
        "            obj={\n",
        "                \"case_id\": case[\"case_id\"],\n",
        "                \"case_title\": case.get(\"case_title\"),\n",
        "                \"n_pages\": case.get(\"n_pages\"),\n",
        "                \"page_paths\": case.get(\"page_paths\"),\n",
        "                \"patient\": demographics or None,\n",
        "                \"presentation\": {\"symptoms\": symptoms or None, \"signs\": signs or None, \"history_text\": hx or None},\n",
        "                \"vitals\": vitals or None,\n",
        "                \"tests\": {\"labs\": labs or None, \"microbiology\": microbiology or None, \"imaging_findings\": imaging or None},\n",
        "                \"diagnoses\": diagnoses or None,\n",
        "                \"management\": management or None,\n",
        "                \"free_text\": {\"discussion\": disc or None, \"summary\": summary or None}\n",
        "            }\n",
        "            fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\"); n+=1\n",
        "    print(f\"[done] structured → {out_jsonl} (cases={n})\")\n",
        "    return out_jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) SFT writers"
      ],
      "metadata": {
        "id": "wfg_rCh7-McY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd448f2c"
      },
      "source": [
        "INSTR_EXTRACT = (\n",
        "    \"Extract the following fields from the clinical case text and return only JSON with keys: \"\n",
        "    \"patient (age, sex, from_location, travel, hiv_status, pregnant), \"\n",
        "    \"presentation (symptoms[], signs[], history_text), vitals, tests (labs, microbiology, imaging_findings), \"\n",
        "    \"diagnoses (provisional, differential, final), management (medications, outcome).\"\n",
        ")\n",
        "\n",
        "def write_sft_extraction(cases_jsonl: str, struct_jsonl: str, out_sft: str, max_ctx_chars: int = 12000) -> str:\n",
        "    n=0\n",
        "    with open(cases_jsonl,\"r\",encoding=\"utf-8\") as fin_cases, \\\n",
        "         open(struct_jsonl,\"r\",encoding=\"utf-8\") as fin_struct, \\\n",
        "         open(out_sft,\"w\",encoding=\"utf-8\") as fout:\n",
        "        for case_line, struct_line in zip(fin_cases, fin_struct):\n",
        "            case = json.loads(case_line)\n",
        "            struct = json.loads(struct_line)\n",
        "            text = (case.get(\"merged\") or {}).get(\"ocr_text\") or \"\"\n",
        "            text = text[:max_ctx_chars]\n",
        "            ex = {\n",
        "                \"case_id\": struct[\"case_id\"],\n",
        "                \"messages\": [\n",
        "                    {\"role\":\"system\",\"content\":\"You are an accurate clinical information extraction model.\"},\n",
        "                    {\"role\":\"user\",\"content\": f\"{INSTR_EXTRACT}\\n\\nCASE TITLE: {case.get('case_title')}\\n\\nTEXT:\\n{text}\"},\n",
        "                    {\"role\":\"assistant\",\"content\": json.dumps({\n",
        "                        k:struct[k] for k in [\"patient\",\"presentation\",\"vitals\",\"tests\",\"diagnoses\",\"management\"]\n",
        "                    }, ensure_ascii=False)}\n",
        "                ]\n",
        "            }\n",
        "            fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\"); n+=1\n",
        "    print(f\"[done] SFT-extract → {out_sft} (pairs={n})\")\n",
        "    return out_sft\n",
        "\n",
        "def write_sft_dx(struct_jsonl: str, out_sft: str) -> str:\n",
        "    n=0\n",
        "    with open(struct_jsonl,\"r\",encoding=\"utf-8\") as fin, open(out_sft,\"w\",encoding=\"utf-8\") as fout:\n",
        "        for line in fin:\n",
        "            s=json.loads(line)\n",
        "            dx=(s.get(\"diagnoses\") or {}).get(\"final\")\n",
        "            if not dx:\n",
        "                continue\n",
        "            prompt={\"role\":\"user\",\"content\":(\n",
        "                \"Given the structured case below, return JSON with keys: final_diagnosis, differentials[], \"\n",
        "                \"key_features, next_tests[].\\n\\nSTRUCTURED_CASE:\\n\"\n",
        "                + json.dumps({k:s[k] for k in [\"patient\",\"presentation\",\"vitals\",\"tests\"]}, ensure_ascii=False)\n",
        "            )}\n",
        "            target={\"final_diagnosis\": dx, \"differentials\": [], \"key_features\": (s.get(\"presentation\") or {}).get(\"symptoms\") or [], \"next_tests\": []}\n",
        "            ex={\"case_id\": s[\"case_id\"], \"messages\":[\n",
        "                {\"role\":\"system\",\"content\":\"You are a careful tropical medicine diagnostician.\"},\n",
        "                prompt, {\"role\":\"assistant\",\"content\": json.dumps(target, ensure_ascii=False)}\n",
        "            ]}\n",
        "            fout.write(json.dumps(ex, ensure_ascii=False)+\"\\n\"); n+=1\n",
        "    print(f\"[done] SFT-dx → {out_sft} (pairs={n})\")\n",
        "    return out_sft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) Case-level Qdrant (one point per case; deterministic IDs; upsert)"
      ],
      "metadata": {
        "id": "PpCZ9rZB-Jur"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a831b13f"
      },
      "source": [
        "def build_case_level_index(cases_jsonl: str, mpd=None):\n",
        "    model, processor, device = mpd if mpd else load_colpali()\n",
        "    client = connect_qdrant_rest()\n",
        "    ensure_collection(client, COLLECTION_CASES)\n",
        "\n",
        "    def load_one_page_image(path: str) -> Image.Image:\n",
        "        if \"#page=\" in path:\n",
        "            pdf, page = path.split(\"#page=\")[0], int(path.split(\"#page=\")[1])\n",
        "            pages = convert_from_path(pdf, dpi=220, first_page=page, last_page=page)\n",
        "            return pages[0]\n",
        "        return load_image(path)\n",
        "\n",
        "    vectors, payloads, ids = [], [], []\n",
        "    with open(cases_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f, desc=\"[case-level] embedding\"):\n",
        "            case = json.loads(line)\n",
        "            imgs = [load_one_page_image(p) for p in case[\"page_paths\"]]\n",
        "\n",
        "            # concatenate sub-vectors across pages → one multivector per case\n",
        "            mv_all = []\n",
        "            for i in range(0, len(imgs), BATCH):\n",
        "                batch_imgs = imgs[i:i+BATCH]\n",
        "                mvs = embed_images(model, processor, device, batch_imgs)  # list of multivectors\n",
        "                for mv in mvs:\n",
        "                    mv_all.extend(mv)\n",
        "\n",
        "            vectors.append(mv_all)  # List[List[float]] with subvector size=128\n",
        "            payloads.append({\n",
        "                \"case_id\": case[\"case_id\"],\n",
        "                \"case_title\": case[\"case_title\"],\n",
        "                \"n_pages\": case[\"n_pages\"],\n",
        "                \"page_paths\": case[\"page_paths\"],\n",
        "                \"modality\": \"case_multivector\",\n",
        "            })\n",
        "            ids.append(f\"case::{case['case_id']}\")\n",
        "\n",
        "    upsert_points(client, COLLECTION_CASES, vectors, payloads, ids)\n",
        "    print(\"[done] case-level collection built →\", COLLECTION_CASES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) Run the pipeline"
      ],
      "metadata": {
        "id": "diprKX4k-G5G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00babbda"
      },
      "source": [
        "def head(path, n=1):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for _ in range(n):\n",
        "                print(f.readline().rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(\"missing:\", path)\n",
        "\n",
        "def main():\n",
        "    client, mpd = build_pages_index(SOURCE_DIR)\n",
        "    cases_path   = merge_pages_to_cases(PAGES_JSONL, CASES_JSONL)\n",
        "    struct_path  = build_structured_cases(CASES_JSONL, STRUCT_JSONL)\n",
        "    write_sft_extraction(CASES_JSONL, STRUCT_JSONL, SFT_EXTRACT_JSONL)\n",
        "    write_sft_dx(STRUCT_JSONL, SFT_DX_JSONL)\n",
        "    if INDEX_CASES:\n",
        "        build_case_level_index(cases_path, mpd=mpd)\n",
        "\n",
        "    print(\"\\n[files]\")\n",
        "    for p in [PAGES_JSONL, CASES_JSONL, STRUCT_JSONL, SFT_EXTRACT_JSONL, SFT_DX_JSONL]:\n",
        "        try:\n",
        "            import subprocess, shlex\n",
        "            out = subprocess.check_output(shlex.split(f\"wc -l {p}\")).decode().strip()\n",
        "            print(out)\n",
        "        except Exception:\n",
        "            print(\"missing:\", p)\n",
        "\n",
        "    print(\"\\n[sample case jsonl]\");       head(CASES_JSONL, 1)\n",
        "    print(\"\\n[sample structured jsonl]\"); head(STRUCT_JSONL, 1)\n",
        "\n",
        "try:\n",
        "    main()\n",
        "except Exception as e:\n",
        "    # No sys.exit; just show the error cleanly\n",
        "    print(\"[fatal]\", repr(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_Z7_-ODXy4M",
        "outputId": "b2f31013-432d-47f3-e6d7-4819dfcee1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
