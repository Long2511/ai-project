{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8FnGPuCnadf0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: LOAD DATA SET & CONSTRUCT ragas_evaluation_dataset.xlsx"
      ],
      "metadata": {
        "id": "lPc3hUtFaXxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Project-AI-Eval/Test-Data/\")\n",
        "OUTPUT_FILE = \"ragas_evaluation_dataset.xlsx\"\n",
        "\n",
        "# --- 2. PROCESSING FUNCTION ---\n",
        "def load_case_data(root_path):\n",
        "    data_entries = []\n",
        "\n",
        "    # Get all subfolders (001, 002, etc.) and sort them\n",
        "    if not root_path.exists():\n",
        "        print(f\"‚ùå Error: Path {root_path} does not exist!\")\n",
        "        return []\n",
        "    case_folders = sorted([f for f in root_path.iterdir() if f.is_dir()])\n",
        "\n",
        "    print(f\"üìÇ Found {len(case_folders)} case folders. Processing...\")\n",
        "\n",
        "    for folder in case_folders:\n",
        "        case_id = folder.name  # e.g., \"001\"\n",
        "\n",
        "        # Paths\n",
        "        query_path = folder / \"query.txt\"\n",
        "        gt_path = folder / \"ground_truth.txt\"\n",
        "        images_dir = folder / \"images\"\n",
        "\n",
        "        # 1. Read Query (User Input)\n",
        "        user_input = \"\"\n",
        "        if query_path.exists():\n",
        "            try:\n",
        "                with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    user_input = f.read().strip()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error reading query for {case_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Missing query.txt for case {case_id}\")\n",
        "            continue\n",
        "\n",
        "        # 2. Read Ground Truth\n",
        "        ground_truth = \"\"\n",
        "        if gt_path.exists():\n",
        "            try:\n",
        "                with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    ground_truth = f.read().strip()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error reading ground_truth for {case_id}: {e}\")\n",
        "\n",
        "        # 3. Process Images (The \"Image Context\" Step)\n",
        "        image_notes = []\n",
        "        if images_dir.exists() and images_dir.is_dir():\n",
        "            # Get valid image files\n",
        "            images = [img.name for img in images_dir.iterdir()\n",
        "                      if img.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n",
        "\n",
        "            if images:\n",
        "                # Sort images to maintain order (e.g. p1 before p2)\n",
        "                images.sort()\n",
        "\n",
        "                # Format: [User provided image: /path/to/image.png]\n",
        "                for img_name in images:\n",
        "                    full_img_path = str(images_dir / img_name)\n",
        "                    image_notes.append(f\"[User provided image: {full_img_path}]\")\n",
        "\n",
        "        # 4. Construct Final User Input for Ragas\n",
        "        # Combine Text Query + Image Contexts\n",
        "        final_user_input = user_input\n",
        "        if image_notes:\n",
        "            final_user_input += \"\\n\\n\" + \"\\n\".join(image_notes)\n",
        "\n",
        "        # 5. Append to List\n",
        "        data_entries.append({\n",
        "            \"case_id\": case_id,\n",
        "            \"user_input\": final_user_input,\n",
        "            \"ground_truth\": ground_truth\n",
        "        })\n",
        "\n",
        "    return data_entries\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "print(\"üöÄ Starting Data Preparation...\")\n",
        "dataset_list = load_case_data(DATA_ROOT / \"extracted-data\")\n",
        "\n",
        "if dataset_list:\n",
        "    df = pd.DataFrame(dataset_list)\n",
        "\n",
        "    # Save to Excel\n",
        "    df.to_excel(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ Success! Processed {len(df)} cases.\")\n",
        "    print(f\"üìÅ Saved to: {OUTPUT_FILE}\")\n",
        "    print(\"\\n--- Preview of Row 0 ---\")\n",
        "    print(df.iloc[0]['user_input'])\n",
        "else:\n",
        "    print(\"‚ùå No data found. Check your folder path.\")"
      ],
      "metadata": {
        "id": "SECaHKFOT2v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472c8a26-09de-4145-c2bf-ae18bd329c68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Data Preparation...\n",
            "üìÇ Found 33 case folders. Processing...\n",
            "\n",
            "‚úÖ Success! Processed 33 cases.\n",
            "üìÅ Saved to: ragas_evaluation_dataset.xlsx\n",
            "\n",
            "--- Preview of Row 0 ---\n",
            "A 53-year-old woman presented with fever, cough, and malaise after returning from a visit to Lahore. On examination, her temperature was 38¬∞C and she had a rash on her upper chest. A chest X-ray showed patchy basal consolidation and a full blood count revealed a relative lymphocytosis. Malaria films were negative. Blood cultures were drawn and later grew gram-negative bacilli.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: APPEND MODEL ANSWER to ragas_evaluation_dataset.xlsx"
      ],
      "metadata": {
        "id": "GH1elxWkhXbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def append_gwen_results(\n",
        "    source_file,\n",
        "    result_file,\n",
        "    model_tag: str,\n",
        "    answer_col: str = \"answer\",\n",
        "    case_id_col: str = \"case_id\",\n",
        "    context_col: str | None = None,   # Pass in context if the model uses RAG\n",
        "):\n",
        "    # -----------------------------\n",
        "    # 0. NORMALIZE INPUTS\n",
        "    # -----------------------------\n",
        "    source_file = Path(source_file)\n",
        "    result_file = Path(result_file)\n",
        "\n",
        "    if not source_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Source file not found: {source_file}\")\n",
        "    if not result_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Result file not found: {result_file}\")\n",
        "\n",
        "    if not re.match(r\"^[a-zA-Z0-9_]+$\", model_tag):\n",
        "        raise ValueError(f\"‚ùå Invalid model_tag: {model_tag}\")\n",
        "\n",
        "    disease_col = f\"{model_tag}_answer_disease\"\n",
        "    reasoning_col = f\"{model_tag}_answer_reasoning\"\n",
        "    context_out_col = f\"{model_tag}_context\" if context_col else None\n",
        "\n",
        "    print(f\"üîß Processing model: {model_tag}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. LOAD SOURCE\n",
        "    # -----------------------------\n",
        "    try:\n",
        "        df_src = pd.read_excel(source_file)\n",
        "    except Exception:\n",
        "        df_src = pd.read_csv(source_file)\n",
        "\n",
        "    for col in [case_id_col, answer_col]:\n",
        "        if col not in df_src.columns:\n",
        "            raise KeyError(f\"‚ùå Column '{col}' missing in {source_file}\")\n",
        "\n",
        "    if context_col and context_col not in df_src.columns:\n",
        "        raise KeyError(f\"‚ùå Context column '{context_col}' missing in {source_file}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. ROBUST EXTRACTION\n",
        "    # -----------------------------\n",
        "    # Paterns of the model's answer\n",
        "    PATTERNS = [\n",
        "        r\"\\*\\*Predicted disease:\\*\\*\\s*(.*?)\\s*\\*\\*Reason:\\*\\*\\s*(.*)\",\n",
        "        r\"Predicted disease:\\s*(.*?)\\s*Reason:\\s*(.*)\",\n",
        "        r\"Predicted disease:\\s*(.*?)\\s*Reasoning:\\s*(.*)\",\n",
        "        r\"Disease:\\s*(.*?)\\s*Reason:\\s*(.*)\",\n",
        "        r\"Diagnosis:\\s*(.*?)\\s*Reason:\\s*(.*)\",\n",
        "        r\"predicted disease is\\s*(.*?)\\s*(?:because|as|due to)\\s*(.*)\",\n",
        "    ]\n",
        "\n",
        "    def extract(text):\n",
        "        text = str(text).strip()\n",
        "        for p in PATTERNS:\n",
        "            m = re.search(p, text, re.IGNORECASE | re.DOTALL)\n",
        "            if m:\n",
        "                return m.group(1).strip(), m.group(2).strip()\n",
        "        return \"\", \"\"\n",
        "\n",
        "    extracted = df_src[answer_col].apply(lambda x: pd.Series(extract(x)))\n",
        "    extracted.columns = [disease_col, reasoning_col]\n",
        "    df_src = pd.concat([df_src, extracted], axis=1)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. SANITY CHECK\n",
        "    # -----------------------------\n",
        "    empty_rate = (df_src[disease_col] == \"\").mean()\n",
        "    if empty_rate > 0.3:\n",
        "        raise ValueError(\n",
        "            f\"‚ùå Extraction failed for {empty_rate:.0%} of rows \"\n",
        "            f\"(format drift or broken regex)\"\n",
        "        )\n",
        "\n",
        "    df_src[case_id_col] = df_src[case_id_col].astype(str)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. LOAD RESULT FILE\n",
        "    # -----------------------------\n",
        "    df_res = pd.read_excel(result_file)\n",
        "    if case_id_col not in df_res.columns:\n",
        "        raise KeyError(f\"‚ùå '{case_id_col}' missing in result file\")\n",
        "\n",
        "    df_res[case_id_col] = df_res[case_id_col].astype(str)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. OVERWRITE EXISTING COLUMNS\n",
        "    # -----------------------------\n",
        "    cols_to_drop = [disease_col, reasoning_col]\n",
        "    if context_out_col:\n",
        "        cols_to_drop.append(context_out_col)\n",
        "\n",
        "    for col in cols_to_drop:\n",
        "        if col in df_res.columns:\n",
        "            print(f\"‚ö†Ô∏è Overwriting existing column: {col}\")\n",
        "            df_res = df_res.drop(columns=[col])\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6. PREP MERGE FRAME\n",
        "    # -----------------------------\n",
        "    merge_cols = [case_id_col, disease_col, reasoning_col]\n",
        "\n",
        "    if context_col:\n",
        "        df_src[context_out_col] = df_src[context_col].astype(str)\n",
        "        merge_cols.append(context_out_col)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 7. MERGE\n",
        "    # -----------------------------\n",
        "    df_final = pd.merge(\n",
        "        df_res,\n",
        "        df_src[merge_cols],\n",
        "        on=case_id_col,\n",
        "        how=\"left\",\n",
        "        validate=\"one_to_one\",\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 8. SAVE\n",
        "    # -----------------------------\n",
        "    df_final.to_excel(result_file, index=False)\n",
        "    print(f\"‚úÖ Appended {model_tag} ‚Üí {result_file}\")\n"
      ],
      "metadata": {
        "id": "W6wZWJWnbwyh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Project-AI-Eval/Test-Data/\")\n",
        "OUTPUT_FILE = \"ragas_evaluation_dataset.xlsx\""
      ],
      "metadata": {
        "id": "4m5cALHtqdIj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Gwen base"
      ],
      "metadata": {
        "id": "F9YL_8O1qAEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "append_gwen_results(\n",
        "    source_file=\"gwen_base.xlsx\",\n",
        "    result_file=OUTPUT_FILE,\n",
        "    model_tag=\"gwen_base\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVNVGd_hqXcd",
        "outputId": "c7d75ac2-4b2c-40ee-8914-11e769fbb540"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gwen_base\n",
            "‚úÖ Appended gwen_base ‚Üí ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Gwen finetune"
      ],
      "metadata": {
        "id": "-eroUOwVqGRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "append_gwen_results(\n",
        "    source_file=\"gwen_finetune.xlsx\",\n",
        "    result_file=OUTPUT_FILE,\n",
        "    model_tag=\"gwen_finetune\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6TApWDpqZ4U",
        "outputId": "c972fa00-417d-4a32-cf80-1050840c4c49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gwen_finetune\n",
            "‚úÖ Appended gwen_finetune ‚Üí ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Gwen RAG"
      ],
      "metadata": {
        "id": "pJ9ClMt6qMHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "append_gwen_results(\n",
        "    source_file=\"gwen_rag.xlsx\",\n",
        "    result_file=OUTPUT_FILE,\n",
        "    model_tag=\"gwen_rag\",\n",
        "    context_col=\"context\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqVytwn9qhD3",
        "outputId": "7b20032b-0dec-41bf-862d-fe0b0075be24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gwen_rag\n",
            "‚úÖ Appended gwen_rag ‚Üí ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Gwen finetune + RAG"
      ],
      "metadata": {
        "id": "KxWKK59chaEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "append_gwen_results(\n",
        "    source_file=\"gwen_finetune_rag_3.xlsx\",\n",
        "    result_file=OUTPUT_FILE,\n",
        "    model_tag=\"gwen_finetune_rag\",\n",
        "    answer_col=\"pred_answer\",\n",
        "    context_col=\"rag_trace\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omv8PHk3fLDM",
        "outputId": "a7594c27-aef2-4ed6-bdc7-4c6c7e44976d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gwen_finetune_rag\n",
            "‚úÖ Appended gwen_finetune_rag ‚Üí ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA 1: LOAD UNTIDY ANSWER\n",
        "load answer where format is not deterministic (e.g. mixing answer and reasoning)"
      ],
      "metadata": {
        "id": "ME3BYT6iY17N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def append_gwen_results(\n",
        "    source_file,\n",
        "    result_file,\n",
        "    model_tag: str,\n",
        "    answer_col: str = \"pred_answer\",\n",
        "    case_id_col: str = \"case_id\",\n",
        "    context_col: str | None = None,\n",
        "):\n",
        "    # -----------------------------\n",
        "    # 0. NORMALIZE INPUTS\n",
        "    # -----------------------------\n",
        "    source_file = Path(source_file)\n",
        "    result_file = Path(result_file)\n",
        "\n",
        "    if not source_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Source file not found: {source_file}\")\n",
        "    if not result_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Result file not found: {result_file}\")\n",
        "\n",
        "    if not re.match(r\"^[a-zA-Z0-9_]+$\", model_tag):\n",
        "        raise ValueError(f\"‚ùå Invalid model_tag: {model_tag}\")\n",
        "\n",
        "    # Define the single output column for the answer\n",
        "    answer_out_col = f\"{model_tag}_answer\"\n",
        "    context_out_col = f\"{model_tag}_context\" if context_col else None\n",
        "\n",
        "    print(f\"üîß Processing model: {model_tag}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. LOAD SOURCE\n",
        "    # -----------------------------\n",
        "    try:\n",
        "        df_src = pd.read_excel(source_file)\n",
        "    except Exception:\n",
        "        df_src = pd.read_csv(source_file)\n",
        "\n",
        "    # Check for required columns\n",
        "    for col in [case_id_col, answer_col]:\n",
        "        if col not in df_src.columns:\n",
        "            raise KeyError(f\"‚ùå Column '{col}' missing in {source_file}\")\n",
        "\n",
        "    if context_col and context_col not in df_src.columns:\n",
        "        raise KeyError(f\"‚ùå Context column '{context_col}' missing in {source_file}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. PREPARE DATA (Simpler Logic)\n",
        "    # -----------------------------\n",
        "    # Ensure ID is string for merging\n",
        "    df_src[case_id_col] = df_src[case_id_col].astype(str)\n",
        "\n",
        "    # Directly copy the answer column content instead of parsing\n",
        "    # We strip whitespace to keep it clean\n",
        "    df_src[answer_out_col] = df_src[answer_col].astype(str).str.strip()\n",
        "\n",
        "    # Handle context if requested\n",
        "    if context_col:\n",
        "        df_src[context_out_col] = df_src[context_col].astype(str)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. LOAD RESULT FILE\n",
        "    # -----------------------------\n",
        "    try:\n",
        "        df_res = pd.read_excel(result_file)\n",
        "    except Exception:\n",
        "        # Fallback if result file is CSV, though usually it's excel per function name\n",
        "        df_res = pd.read_csv(result_file)\n",
        "\n",
        "    if case_id_col not in df_res.columns:\n",
        "        raise KeyError(f\"‚ùå '{case_id_col}' missing in result file\")\n",
        "\n",
        "    df_res[case_id_col] = df_res[case_id_col].astype(str)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. OVERWRITE EXISTING COLUMNS\n",
        "    # -----------------------------\n",
        "    # Drop the specific columns we are about to add if they already exist\n",
        "    cols_to_drop = [answer_out_col]\n",
        "    if context_out_col:\n",
        "        cols_to_drop.append(context_out_col)\n",
        "\n",
        "    for col in cols_to_drop:\n",
        "        if col in df_res.columns:\n",
        "            print(f\"‚ö†Ô∏è Overwriting existing column: {col}\")\n",
        "            df_res = df_res.drop(columns=[col])\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. PREP MERGE FRAME\n",
        "    # -----------------------------\n",
        "    merge_cols = [case_id_col, answer_out_col]\n",
        "    if context_out_col:\n",
        "        merge_cols.append(context_out_col)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6. MERGE\n",
        "    # -----------------------------\n",
        "    df_final = pd.merge(\n",
        "        df_res,\n",
        "        df_src[merge_cols],\n",
        "        on=case_id_col,\n",
        "        how=\"left\",\n",
        "        validate=\"one_to_one\",\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 7. SAVE\n",
        "    # -----------------------------\n",
        "    df_final.to_excel(result_file, index=False)\n",
        "    print(f\"‚úÖ Appended {model_tag} ‚Üí {result_file}\")"
      ],
      "metadata": {
        "id": "NLO4QM0kY1RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "append_gwen_results(\n",
        "    source_file='gwen_finetune_rag_4.xlsx',\n",
        "    result_file='ragas_evaluation_dataset_finetune_rag_4.xlsx',\n",
        "    model_tag=\"gwen_finetune_rag_4\",          # This prefix will be added to columns (e.g. gwen_rag_answer)\n",
        "    case_id_col=\"test_id\",         # Matches the ID column in your CSV\n",
        "    answer_col=\"pred_answer\",      # The column to grab as the Answer\n",
        "    context_col=\"rag_trace\"        # The column to grab as the Context\n",
        ")"
      ],
      "metadata": {
        "id": "ZWzW7wgIacbe",
        "outputId": "3fd97ff9-488b-4c2d-85d4-b995f3f434c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gwen_ft_rag_4\n",
            "‚úÖ Appended gwen_ft_rag_4 ‚Üí ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA 2: LOAD SEPARATED DISEASE / REASONING"
      ],
      "metadata": {
        "id": "b6gsORPCNM3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def append_results(\n",
        "    source_file,\n",
        "    result_file,\n",
        "    model_tag: str,\n",
        "    source_disease_col: str = \"disease\",\n",
        "    source_reasoning_col: str = \"reasoning\",\n",
        "    case_id_col: str = \"case_id\",\n",
        "    source_context_col: str | None = \"context\",  # Pass None if no context\n",
        "):\n",
        "    \"\"\"\n",
        "    Appends disease and reasoning results from a source file to a master result file.\n",
        "    No regex extraction is performed; columns are read directly.\n",
        "    \"\"\"\n",
        "    # -----------------------------\n",
        "    # 0. NORMALIZE INPUTS\n",
        "    # -----------------------------\n",
        "    source_file = Path(source_file)\n",
        "    result_file = Path(result_file)\n",
        "\n",
        "    if not source_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Source file not found: {source_file}\")\n",
        "    if not result_file.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Result file not found: {result_file}\")\n",
        "\n",
        "    if not re.match(r\"^[a-zA-Z0-9_]+$\", model_tag):\n",
        "        raise ValueError(f\"‚ùå Invalid model_tag: {model_tag}\")\n",
        "\n",
        "    # Define output column names for the result file\n",
        "    out_disease_col = f\"{model_tag}_answer_disease\"\n",
        "    out_reasoning_col = f\"{model_tag}_answer_reasoning\"\n",
        "    out_context_col = f\"{model_tag}_context\" if source_context_col else None\n",
        "\n",
        "    print(f\"üîß Processing model: {model_tag}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. LOAD SOURCE\n",
        "    # -----------------------------\n",
        "    # Detect format based on extension\n",
        "    if source_file.suffix.lower() == '.csv':\n",
        "        df_src = pd.read_csv(source_file)\n",
        "    else:\n",
        "        df_src = pd.read_excel(source_file)\n",
        "\n",
        "    # Check for required columns\n",
        "    required = [case_id_col, source_disease_col, source_reasoning_col]\n",
        "    if source_context_col:\n",
        "        required.append(source_context_col)\n",
        "\n",
        "    for col in required:\n",
        "        if col not in df_src.columns:\n",
        "            raise KeyError(f\"‚ùå Column '{col}' missing in {source_file.name}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. PREPARE DATA FOR MERGE\n",
        "    # -----------------------------\n",
        "    # Select and rename columns to the final output format\n",
        "    rename_map = {\n",
        "        source_disease_col: out_disease_col,\n",
        "        source_reasoning_col: out_reasoning_col\n",
        "    }\n",
        "    if source_context_col:\n",
        "        rename_map[source_context_col] = out_context_col\n",
        "\n",
        "    df_to_merge = df_src[[case_id_col] + list(rename_map.keys())].copy()\n",
        "    df_to_merge = df_to_merge.rename(columns=rename_map)\n",
        "\n",
        "    # Ensure ID types match (cast to string for safe merging)\n",
        "    df_to_merge[case_id_col] = df_to_merge[case_id_col].astype(str)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. LOAD & CLEAN RESULT FILE\n",
        "    # -----------------------------\n",
        "    if result_file.suffix.lower() == '.csv':\n",
        "        df_res = pd.read_csv(result_file)\n",
        "    else:\n",
        "        df_res = pd.read_excel(result_file)\n",
        "\n",
        "    if case_id_col not in df_res.columns:\n",
        "        raise KeyError(f\"‚ùå '{case_id_col}' missing in result file\")\n",
        "\n",
        "    df_res[case_id_col] = df_res[case_id_col].astype(str)\n",
        "\n",
        "    # Drop existing columns if we are overwriting them\n",
        "    cols_to_drop = [out_disease_col, out_reasoning_col]\n",
        "    if out_context_col:\n",
        "        cols_to_drop.append(out_context_col)\n",
        "\n",
        "    for col in cols_to_drop:\n",
        "        if col in df_res.columns:\n",
        "            print(f\"‚ö†Ô∏è Overwriting existing column: {col}\")\n",
        "            df_res = df_res.drop(columns=[col])\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. MERGE & SAVE\n",
        "    # -----------------------------\n",
        "    df_final = pd.merge(\n",
        "        df_res,\n",
        "        df_to_merge,\n",
        "        on=case_id_col,\n",
        "        how=\"left\",\n",
        "        validate=\"one_to_one\"\n",
        "    )\n",
        "\n",
        "    if result_file.suffix.lower() == '.csv':\n",
        "        df_final.to_csv(result_file, index=False)\n",
        "    else:\n",
        "        df_final.to_excel(result_file, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Successfully appended {model_tag} to {result_file.name}\")"
      ],
      "metadata": {
        "id": "RYWbo6JWNV7C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "append_results(\n",
        "    source_file=\"gemini_answer.xlsx\",\n",
        "    result_file=\"ragas_evaluation_dataset.xlsx\",\n",
        "    model_tag=\"gemini\",\n",
        "    source_disease_col=\"disease\",\n",
        "    source_reasoning_col=\"reasoning\",\n",
        "    source_context_col=\"context\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF2R0XRFOOHr",
        "outputId": "98eb0834-f7bb-4335-c411-cf942f369d57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processing model: gemini\n",
            "‚úÖ Successfully appended gemini to ragas_evaluation_dataset.xlsx\n"
          ]
        }
      ]
    }
  ]
}